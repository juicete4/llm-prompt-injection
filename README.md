# Security in the Age of Large Language Models: Investigating the robustness of LLMs' Safety Alignment and Privacy via Direct and Indirect Prompt Injection

This repository contains the results and datasets used in my research thesis focusing on evaluating the resistance to prompt injection attacks of the following LLMs:
* GPT-3.5	(gpt-3.5-turbo-0613)
* GPT-4	(gpt-4-1106-preview)
* PaLM 2 (text-bison-001)
* Gemini Pro (gemini-1.0-pro-001)
* Llama 2 (llama-2-70b-chat)
				
For the full detail of methods and experiments, please refer to the full text of the thesis.


> Disclaimer. The documents in this repository contain examples of harmful language. Reader discretion is recommended.
